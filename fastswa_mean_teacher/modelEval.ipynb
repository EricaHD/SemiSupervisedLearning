{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets\n",
    "import torchvision\n",
    "\n",
    "from mean_teacher import datasets, architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = datasets.__dict__['sslMini']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model_arch, pretrained_model_path, cuda=True):\n",
    "        # Load pretrained model\n",
    "        pretrained_model = torch.load(f=pretrained_model_path, map_location=\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in pretrained_model['state_dict'].items():\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        # Load pre-trained weights in current model\n",
    "        with torch.no_grad():\n",
    "            model_arch.load_state_dict(new_state_dict, strict=True)\n",
    "\n",
    "        # Debug loading\n",
    "        #print('Parameters found in pretrained model:')\n",
    "        pretrained_layers = new_state_dict.keys()\n",
    "        #for l in pretrained_layers:\n",
    "        #    print('\\t' + l)\n",
    "        #print('')\n",
    "\n",
    "        for name, module in model_arch.state_dict().items():\n",
    "            if name in pretrained_layers:\n",
    "                assert torch.equal(new_state_dict[name].cpu(), module.cpu())\n",
    "                #print('{} have been loaded correctly in current model.'.format(name))\n",
    "            else:\n",
    "                raise ValueError(\"state_dict() keys do not match\")\n",
    "                \n",
    "        return model_arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldir = \"/scratch/ijh216/ssl_mini/supervised/val\"\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(evaldir, dataset_config['eval_transformation']),\n",
    "                                              batch_size=32,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=2,  # Needs images twice as fast\n",
    "                                              #pin_memory=True,\n",
    "                                              drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/scratch/ijh216/ssl/ssl_shake_mini/2019-05-01_19-04-25/10/transient/checkpoint.230.ckpt\" \n",
    "model = architectures.__dict__['cifar_shakeshake26']().to(device)\n",
    "model = load_weights(model, model_dir, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 0.\n",
    "n_correct_top_1 = 0\n",
    "n_correct_top_k = 0\n",
    "\n",
    "for img, target in eval_loader:\n",
    "    img, target = img.to(device), target.to(device)\n",
    "    batch_size = img.size(0)\n",
    "    n_samples += batch_size\n",
    "\n",
    "        # Forward\n",
    "    output = model(img)[0]\n",
    "\n",
    "        # Top 1 accuracy\n",
    "    pred_top_1 = torch.topk(output, k=1, dim=1)[1]\n",
    "    n_correct_top_1 += pred_top_1.eq(target.view_as(pred_top_1)).int().sum().item()\n",
    "\n",
    "        # Top k accuracy\n",
    "    pred_top_k = torch.topk(output, k=5, dim=1)[1]\n",
    "    target_top_k = target.view(-1, 1).expand(32, 5)\n",
    "    n_correct_top_k += pred_top_k.eq(target_top_k).int().sum().item()\n",
    "\n",
    "    # Accuracy\n",
    "top_1_acc = n_correct_top_1/n_samples\n",
    "top_k_acc = n_correct_top_k/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(inp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_top_1 = torch.topk(out, k=1, dim=1)[1]\n",
    "n_correct_top_1 = pred_top_1.eq(target.view_as(pred_top_1)).int().sum().item()\n",
    "\n",
    "        # Top k accuracy\n",
    "pred_top_k = torch.topk(out, k=5, dim=1)[1]\n",
    "target_top_k = target.view(-1, 1).expand(32, 5)\n",
    "n_correct_top_k = pred_top_k.eq(target_top_k).int().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[173, 175, 989, 166, 995],\n",
       "        [299, 362, 472, 685, 293],\n",
       "        [573, 606, 721, 835, 145],\n",
       "        [842, 257, 262, 403,  67],\n",
       "        [325, 745,  54,  67, 266],\n",
       "        [278, 371, 993, 460, 431],\n",
       "        [117, 993, 730, 703, 561],\n",
       "        [108, 344, 322,  44, 201],\n",
       "        [144, 216, 823, 541, 725],\n",
       "        [114, 241, 873, 498, 178],\n",
       "        [320, 542, 510, 630, 545],\n",
       "        [505, 366, 489, 698, 339],\n",
       "        [528, 297, 728, 413, 540],\n",
       "        [588, 806, 801, 585, 500],\n",
       "        [772, 656, 594, 785, 214],\n",
       "        [186, 297, 410, 846, 694],\n",
       "        [335, 564, 363, 286, 419],\n",
       "        [568, 342, 471, 238,  82],\n",
       "        [ 84, 386, 336, 846, 541],\n",
       "        [525, 432, 641, 565, 233],\n",
       "        [242, 188, 193, 389, 113],\n",
       "        [114, 312, 730,  89,  77],\n",
       "        [693, 568, 792, 160, 754],\n",
       "        [138, 289, 970, 594, 842],\n",
       "        [145, 729, 512, 792, 715],\n",
       "        [ 84,   2, 328, 281, 832],\n",
       "        [459, 946, 521,  29, 819],\n",
       "        [423, 496, 387, 683, 570],\n",
       "        [150,  47,  71, 536,  65],\n",
       "        [734, 921, 919, 936, 116],\n",
       "        [448,  27, 263, 476,   2],\n",
       "        [197, 199, 146, 208, 187]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(out, k=5, dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.]), tensor([0.])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(out, target, topk=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[173, 175, 989, 166, 995],\n",
       "        [299, 362, 472, 685, 293],\n",
       "        [573, 606, 721, 835, 145],\n",
       "        [842, 257, 262, 403,  67],\n",
       "        [325, 745,  54,  67, 266],\n",
       "        [278, 371, 993, 460, 431],\n",
       "        [117, 993, 730, 703, 561],\n",
       "        [108, 344, 322,  44, 201],\n",
       "        [144, 216, 823, 541, 725],\n",
       "        [114, 241, 873, 498, 178],\n",
       "        [320, 542, 510, 630, 545],\n",
       "        [505, 366, 489, 698, 339],\n",
       "        [528, 297, 728, 413, 540],\n",
       "        [588, 806, 801, 585, 500],\n",
       "        [772, 656, 594, 785, 214],\n",
       "        [186, 297, 410, 846, 694],\n",
       "        [335, 564, 363, 286, 419],\n",
       "        [568, 342, 471, 238,  82],\n",
       "        [ 84, 386, 336, 846, 541],\n",
       "        [525, 432, 641, 565, 233],\n",
       "        [242, 188, 193, 389, 113],\n",
       "        [114, 312, 730,  89,  77],\n",
       "        [693, 568, 792, 160, 754],\n",
       "        [138, 289, 970, 594, 842],\n",
       "        [145, 729, 512, 792, 715],\n",
       "        [ 84,   2, 328, 281, 832],\n",
       "        [459, 946, 521,  29, 819],\n",
       "        [423, 496, 387, 683, 570],\n",
       "        [150,  47,  71, 536,  65],\n",
       "        [734, 921, 919, 936, 116],\n",
       "        [448,  27, 263, 476,   2],\n",
       "        [197, 199, 146, 208, 187]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.topk(5, 1, True, True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "?out.topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
