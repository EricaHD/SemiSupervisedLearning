{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from mean_teacher import datasets, architectures\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "to_image = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = datasets.__dict__['sslMini']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model_arch, pretrained_model_path, state_dict, cuda=True):\n",
    "        # Load pretrained model\n",
    "        pretrained_model = torch.load(f=pretrained_model_path, map_location=\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in pretrained_model[state_dict].items():\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        # Load pre-trained weights in current model\n",
    "        with torch.no_grad():\n",
    "            model_arch.load_state_dict(new_state_dict, strict=True)\n",
    "\n",
    "        # Debug loading\n",
    "        #print('Parameters found in pretrained model:')\n",
    "        pretrained_layers = new_state_dict.keys()\n",
    "        #for l in pretrained_layers:\n",
    "        #    print('\\t' + l)\n",
    "        #print('')\n",
    "\n",
    "        for name, module in model_arch.state_dict().items():\n",
    "            if name in pretrained_layers:\n",
    "                assert torch.equal(new_state_dict[name].cpu(), module.cpu())\n",
    "                #print('{} have been loaded correctly in current model.'.format(name))\n",
    "            else:\n",
    "                raise ValueError(\"state_dict() keys do not match\")\n",
    "                \n",
    "        return model_arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "evaldir = \"/scratch/ijh216/ssl_mini/supervised/val\"\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(evaldir, dataset_config['eval_transformation']),\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2,\n",
    "                                              #pin_memory=True,\n",
    "                                              drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/scratch/ijh216/ssl/ssl_shake_mini_augment/2019-05-06_18-04-18/10/transient/checkpoint.325.ckpt\" \n",
    "model = architectures.__dict__['cifar_shakeshake26']().to(device)\n",
    "model = load_weights(model, model_dir, state_dict=\"ema_state_dict\", cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    labeled_minibatch_size = max(target.ne(-1).sum(), 1e-8).item()\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True).item()\n",
    "        print(correct_k)\n",
    "        print(correct_k * (100.0 / labeled_minibatch_size))\n",
    "        res.append(correct_k * (100.0 / labeled_minibatch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Acc@1 0.265625\n",
      "Acc@5 0.5\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.30089727722772275\n",
      "Acc@5 0.536355198019802\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.3004508706467662\n",
      "Acc@5 0.5382462686567164\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.30424626245847175\n",
      "Acc@5 0.5367005813953488\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.3063045511221945\n",
      "Acc@5 0.5393157730673317\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.3060625\n",
      "Acc@5 0.539625\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "n_samples = 0.\n",
    "n_correct_top_1 = 0\n",
    "n_correct_top_k = 0\n",
    "\n",
    "for i, (img, target) in enumerate(eval_loader):\n",
    "    img, target = img.to(device), target.to(device)\n",
    "    n_samples += BATCH_SIZE\n",
    "\n",
    "        # Forward\n",
    "    output = model(img)[0]\n",
    "\n",
    "        # Top 1 accuracy\n",
    "    pred_top_1 = torch.topk(output, k=1, dim=1)[1]\n",
    "    n_correct_top_1 += pred_top_1.eq(target.view_as(pred_top_1)).int().sum().item()\n",
    "\n",
    "        # Top k accuracy\n",
    "    pred_top_k = torch.topk(output, k=5, dim=1)[1]\n",
    "    target_top_k = target.view(-1, 1).expand(BATCH_SIZE, 5)\n",
    "    n_correct_top_k += pred_top_k.eq(target_top_k).int().sum().item()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"******************************\")\n",
    "        print(\"Acc@1\", n_correct_top_1/n_samples)\n",
    "        print(\"Acc@5\", n_correct_top_k/n_samples)\n",
    "        print(\"******************************\")\n",
    "    \n",
    "    # Accuracy\n",
    "top_1_acc = n_correct_top_1/n_samples\n",
    "top_k_acc = n_correct_top_k/n_samples\n",
    "\n",
    "print(\"******************************\")\n",
    "print(\"Acc@1\", top_1_acc)\n",
    "print(\"Acc@5\", top_k_acc)\n",
    "print(\"******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Acc@1 0.125\n",
      "Acc@5 0.234375\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.1639851485148515\n",
      "Acc@5 0.3103341584158416\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.16752176616915423\n",
      "Acc@5 0.31467661691542287\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.16715116279069767\n",
      "Acc@5 0.3136939368770764\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.1684071072319202\n",
      "Acc@5 0.31495480049875313\n",
      "******************************\n",
      "******************************\n",
      "Acc@1 0.16746875\n",
      "Acc@5 0.31615625\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "n_samples = 0.\n",
    "n_correct_top_1 = 0\n",
    "n_correct_top_k = 0\n",
    "\n",
    "for i, (img, target) in enumerate(eval_loader):\n",
    "    img, target = img.to(device), target.to(device)\n",
    "    n_samples += BATCH_SIZE\n",
    "\n",
    "        # Forward\n",
    "    output = model(img)[0]\n",
    "\n",
    "        # Top 1 accuracy\n",
    "    pred_top_1 = torch.topk(output, k=1, dim=1)[1]\n",
    "    n_correct_top_1 += pred_top_1.eq(target.view_as(pred_top_1)).int().sum().item()\n",
    "\n",
    "        # Top k accuracy\n",
    "    pred_top_k = torch.topk(output, k=5, dim=1)[1]\n",
    "    target_top_k = target.view(-1, 1).expand(BATCH_SIZE, 5)\n",
    "    n_correct_top_k += pred_top_k.eq(target_top_k).int().sum().item()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"******************************\")\n",
    "        print(\"Acc@1\", n_correct_top_1/n_samples)\n",
    "        print(\"Acc@5\", n_correct_top_k/n_samples)\n",
    "        print(\"******************************\")\n",
    "    \n",
    "    # Accuracy\n",
    "top_1_acc = n_correct_top_1/n_samples\n",
    "top_k_acc = n_correct_top_k/n_samples\n",
    "\n",
    "print(\"******************************\")\n",
    "print(\"Acc@1\", top_1_acc)\n",
    "print(\"Acc@5\", top_k_acc)\n",
    "print(\"******************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
